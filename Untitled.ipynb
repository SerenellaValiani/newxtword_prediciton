{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "owned-style",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sere/work/jupyter/maildir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-51262f16cfda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#vo nella directory con tutti i nomi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/sere/work/jupyter/maildir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#lista dei proprietari delle mail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sere/work/jupyter/maildir'"
     ]
    }
   ],
   "source": [
    "#---PASSO 1---\n",
    "# Carico tutti i contenuti della mail in un unico file \"output_file.txt\"\n",
    "import os\n",
    "import re\n",
    "\n",
    "#apro il file di output\n",
    "out_file = open('output_file.txt', 'w')\n",
    "index = 0\n",
    "\n",
    "#vo nella directory con tutti i nomi\n",
    "os.chdir('/home/sere/work/jupyter/maildir')\n",
    "#lista dei proprietari delle mail\n",
    "dir_name = os.listdir()\n",
    "dir_name.sort()\n",
    "\n",
    "for k in range(len(dir_name)):\n",
    "    os.chdir('/home/sere/work/jupyter/maildir/' + dir_name[k])\n",
    "    #lista delle sottocartelle (es: inbox, trash etc)\n",
    "    sub_dir = os.listdir()\n",
    "\n",
    "    for i in range(len(sub_dir)):\n",
    "        os.chdir('/home/sere/work/jupyter/maildir/' + dir_name[k] + '/' + sub_dir[i])\n",
    "        #lista dei documenti delle sotto-sotto cartelle (es. tutte i nomi delle mail in inbox)\n",
    "        # sono listate come 1. 2. 3. etc\n",
    "        docs = os.listdir()\n",
    "\n",
    "        for doc in docs:\n",
    "            #stampo il percorso in cui mi trovo\n",
    "            print(os.getcwd() + '/' + doc)\n",
    "            \n",
    "            #apro la prima mail\n",
    "            file = open(doc, 'r')\n",
    "\n",
    "            #creo una lista con tutte le righe della prima mail\n",
    "            lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                if line[0:10] == 'X-FileName:':\n",
    "                    index = lines.index(line)\n",
    "\n",
    "            #scrivo su file tutte le righe da dove inizia il contenuto della mail fino alla fine\n",
    "            for el in lines[index + 1:len(lines)]:\n",
    "                out_file.write(el)\n",
    "\n",
    "            #chiudo il file in input\n",
    "            file.close()\n",
    "\n",
    "#chiudo il file di output\n",
    "out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "opponent-formation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file lenght: 2893400\n",
      "Text file cleaned.\n"
     ]
    }
   ],
   "source": [
    "#---PASSO 2---\n",
    "# Pulisco le mail caricate sul file, eliminando tutte le righe che iniziano con \"Date:\", \"From:\" etc.\n",
    "# Salvo il documento pulito su \"output_file_def.txt\"\n",
    "\n",
    "#torno alla directory base\n",
    "os.chdir('/home/sere/work/jupyter/')\n",
    "\n",
    "#apro il file di output\n",
    "file = open('output_file.txt', 'r')\n",
    "out_file = open('output_file_def.txt', 'w')\n",
    "\n",
    "lines = file.readlines()\n",
    "print(\"Input file lenght: \" + str(len(lines)))\n",
    "\n",
    "for line in lines:\n",
    "    if line[0:4] == 'Date' or line[0:4] == 'From' or line[0:2] == 'To' or line[0:7] == 'Subject' or line[0:12] == 'Mime-Version' or line[0:12] == 'Content-Type' or line[0:25] == 'Content-Transfer-Encoding' or line[0:6] == 'X-From' or line[0:4] == 'X-To' or line[0:4] == 'X-cc' or line[0:5] == 'X-bcc' or line[0:8] == 'X-Folder' or line[0:10] == 'X-Original' or line[0:10] == 'X-FileName':\n",
    "        continue\n",
    "    else:\n",
    "        out_file.write(line)\n",
    "\n",
    "\n",
    "print(\"Text file cleaned.\")\n",
    "\n",
    "#chiudo il file di output\n",
    "out_file.close()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accepting-punishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not cleaned file lenght: 2893400\n",
      "Cleaned file lenght: 2191508\n"
     ]
    }
   ],
   "source": [
    "#---PASSO 3---\n",
    "# Controllo che effetivamente siano state eliminate delle righe nel Passo 2\n",
    "\n",
    "#apro il file di output\n",
    "file1 = open('output_file.txt', 'r')\n",
    "file2 = open('output_file_def.txt', 'r')\n",
    "\n",
    "lines1 = file1.readlines()\n",
    "lines2 = file2.readlines()\n",
    "print(\"Not cleaned file lenght: \" + str(len(lines1)))\n",
    "print(\"Cleaned file lenght: \" + str(len(lines2)))\n",
    "\n",
    "out_file.close()\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "altered-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "prefixes = ['an', 're', 'in', 'im', 'dis', 'dif', 'en', 'em', 'pre', 'mis', 'a']\n",
    "suffixes = ['s', 'es', 'ed', 'ing', 'ly', 'er', 'or', 'tion', 'sion', 'able', 'ible', 'al', 'ial']\n",
    "print(prefixes[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "balanced-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "1808\n",
      "mourned\n",
      "\n",
      "=====================================\n",
      "First dict: done\n",
      "Second dict: done\n",
      "Third dict: done\n",
      "Fourth dict: done\n",
      "Fifth dict: done\n"
     ]
    }
   ],
   "source": [
    "#---PASSO 4---\n",
    "# Creo un dataset per addestrare la rete successivamente\n",
    "# Il dataset sara` un dizionario 'tupla -> lista di parole'\n",
    "# Nella lista saranno inserite da 1 a 5 parole, mentre il valore sara` la parola associata\n",
    "import sys\n",
    "#print(sys.path)\n",
    "#sys.executable\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#apro il file\n",
    "#file = open('output_file_def.txt', 'r')\n",
    "file = open('testo_breve.txt', 'r')\n",
    "\n",
    "#regex mi servira` per sostituire la punteggiatura\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "#lista delle righe del file\n",
    "lines = []\n",
    "lines = file.readlines()\n",
    "\n",
    "\n",
    "#stringa delle righe del file\n",
    "data = ''\n",
    "data = ' '.join(lines)\n",
    "#rimuovo i caratteri speciali (piu` — perche` ho notato che rimaneva fuori)\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('—', ' ')\n",
    "\n",
    "\n",
    "#splitto le frasi delimitate dai punti, dalle virgole, dai :, dal ;, dalle \", dal ?, dal !\n",
    "#ottengo una lista di frasi\n",
    "sents = re.split('\\.|,|!|\\?|:|;|\\\"|\\(|\\)|-', data)\n",
    "#sents.remove('  ')\n",
    "sents.remove(' ')\n",
    "sentences = []\n",
    "for s in sents:\n",
    "    sentences.append(s.strip().lower())\n",
    "sentences = [s for s in sentences if s]\n",
    "\n",
    "\n",
    "#creo un vocabolario di tutte le parole usate\n",
    "#rimuovo la punteggiatura\n",
    "nopu_data = regex.sub(' ', data)\n",
    "#considero solo le parole in minuscolo\n",
    "#e` un errore, successivamente pensavo di cambiarle inserendo nomi propri, di citta`, di nazionalita` e sigle in una lista\n",
    "#excp = [I, London, English, American, U.S.A, UK]\n",
    "nopu_data = nopu_data.lower()\n",
    "#lista delle parole (uniche) usate nel testo\n",
    "vocab = []\n",
    "vocab.append('')\n",
    "for i in nopu_data.split():\n",
    "    if i not in vocab:\n",
    "        vocab.append(i)\n",
    "print('=====================================')\n",
    "print(len(vocab))\n",
    "print(vocab[len(vocab) - 1])\n",
    "print(vocab[0])\n",
    "print('=====================================')\n",
    "#stringa delle parole (uniche) usate nel testo\n",
    "string_vocab = ' '.join(vocab)\n",
    "        \n",
    "#tokenizzazione\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([string_vocab])\n",
    "\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "sequence_vocab = tokenizer.texts_to_sequences([string_vocab])[0]\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#print('=======================================')\n",
    "#print(vocab)\n",
    "#print(string_vocab[0:300])\n",
    "#print(sequence_vocab[0:50])\n",
    "#print(type(vocab))\n",
    "#print(type(sequence_vocab))\n",
    "#print(vocab_size)\n",
    "#print('=======================================')\n",
    "\n",
    "#creo un dizionario: tupla -> lista\n",
    "#                    tupla, len 5: ci sono le parole lette in input\n",
    "#                    lista, len variabile: parole in output\n",
    "\n",
    "#DIZIONARIO 1\n",
    "next_word1 = {}\n",
    "for v in vocab:\n",
    "    value_list = []\n",
    "    for s in sentences:\n",
    "        s_list = s.split()\n",
    "        if v in s_list:\n",
    "            index = s_list.index(v)\n",
    "            if index < len(s_list) - 1:\n",
    "                value_list.append(s_list[index + 1])\n",
    "    next_word1[(v, '', '', '', '')] = value_list\n",
    "\n",
    "del_keys = []\n",
    "for key, value in next_word1.items():\n",
    "    if value == []:\n",
    "        del_keys.append(key)\n",
    "for d in del_keys:\n",
    "    del next_word1[d]\n",
    "\n",
    "#for key, value in next_word1.items():\n",
    "#    print(key, value)\n",
    "print('First dict: done')\n",
    "    \n",
    "#DIZIONARIO 2\n",
    "next_word2 = {}                    \n",
    "for key in next_word1:\n",
    "    for v in next_word1[key]:\n",
    "        value_list = []\n",
    "        if (key[0], v, '', '', '') in next_word2.keys():\n",
    "            continue\n",
    "        for s in sentences:\n",
    "            s_list = s.split()\n",
    "            if key[0] in s_list:\n",
    "                index = s_list.index(key[0])\n",
    "                if index < len(s_list) - 2:\n",
    "                    if v == s_list[index + 1]:\n",
    "                        value_list.append(s_list[index + 2])\n",
    "        next_word2[(key[0], v, '', '', '')] = value_list\n",
    "        \n",
    "\n",
    "\n",
    "del_keys = []\n",
    "for key, value in next_word2.items():\n",
    "    if value == []:\n",
    "        del_keys.append(key)\n",
    "for d in del_keys:\n",
    "    del next_word2[d]\n",
    "      \n",
    "#for key, value in next_word2.items():\n",
    "#    print(key, value)\n",
    "print('Second dict: done')\n",
    "\n",
    "#DIZIONARIO 3\n",
    "next_word3 = {}\n",
    "for key in next_word2:\n",
    "    for v in next_word2[key]:\n",
    "        value_list = []\n",
    "        if (key[0], key[1], v, '', '') in next_word3.keys():\n",
    "            continue\n",
    "        for s in sentences:\n",
    "            s_list = s.split()\n",
    "            if key[0] in s_list:\n",
    "                index = s_list.index(key[0])\n",
    "                if index < len(s_list) - 3:\n",
    "                    if key[1] == s_list[index + 1]:\n",
    "                        if v == s_list[index + 2]:\n",
    "                            value_list.append(s_list[index + 3])\n",
    "        next_word3[(key[0], key[1], v, '', '')] = value_list\n",
    "\n",
    "del_keys = []\n",
    "for key, value in next_word3.items():\n",
    "    if value == []:\n",
    "        del_keys.append(key)\n",
    "for d in del_keys:\n",
    "    del next_word3[d]\n",
    "      \n",
    "#for key, value in next_word3.items():\n",
    "#    print(key, value)\n",
    "print('Third dict: done')    \n",
    "    \n",
    "#DIZIONARIO 4\n",
    "next_word4 = {}\n",
    "for key in next_word3:\n",
    "    for v in next_word3[key]:\n",
    "        value_list = []\n",
    "        if (key[0], key[1], key[2], v, '') in next_word4.keys():\n",
    "            continue\n",
    "        for s in sentences:\n",
    "            s_list = s.split()\n",
    "            if key[0] in s_list:\n",
    "                index = s_list.index(key[0])\n",
    "                if index < len(s_list) - 4:\n",
    "                    if key[1] == s_list[index + 1]:\n",
    "                        if key[2] == s_list[index + 2]:\n",
    "                            if v == s_list[index + 3]:\n",
    "                                value_list.append(s_list[index + 4])\n",
    "        next_word4[(key[0], key[1], key[2], v, '')] = value_list\n",
    "\n",
    "del_keys = []\n",
    "for key, value in next_word4.items():\n",
    "    if value == []:\n",
    "        del_keys.append(key)\n",
    "for d in del_keys:\n",
    "    del next_word4[d]\n",
    "      \n",
    "#for key, value in next_word4.items():\n",
    "#    print(key, value)\n",
    "print('Fourth dict: done')\n",
    "\n",
    "#DIZIONARIO 5\n",
    "next_word5 = {}\n",
    "for key in next_word4:\n",
    "    for v in next_word4[key]:\n",
    "        value_list = []\n",
    "        if (key[0], key[1], key[2], key[3], v) in next_word5.keys():\n",
    "            continue\n",
    "        for s in sentences:\n",
    "            s_list = s.split()\n",
    "            if key[0] in s_list:\n",
    "                index = s_list.index(key[0])\n",
    "                if index < len(s_list) - 5:\n",
    "                    if key[1] == s_list[index + 1]:\n",
    "                        if key[2] == s_list[index + 2]:\n",
    "                            if key[3] == s_list[index + 3]:\n",
    "                                if v == s_list[index + 4]:\n",
    "                                    value_list.append(s_list[index + 5])\n",
    "        next_word5[(key[0], key[1], key[2], key[3], v)] = value_list\n",
    "\n",
    "del_keys = []\n",
    "for key, value in next_word5.items():\n",
    "    if value == []:\n",
    "        del_keys.append(key)\n",
    "for d in del_keys:\n",
    "    del next_word5[d]\n",
    "      \n",
    "#for key, value in next_word5.items():\n",
    "#    print(key, value)\n",
    "print('Fifth dict: done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "genetic-norwegian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#PASSO 5\n",
    "#salvo il dataset in un .csv\n",
    "import csv\n",
    "\n",
    "data_set = open('dataset.csv', 'w', newline='')\n",
    "writer = csv.writer(data_set)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for key, value in next_word1.items():\n",
    "    for i in range(len(value)):\n",
    "        writer.writerow([[vocab.index(key[0]), 0, 0, 0, 0], vocab.index(value[i])])\n",
    "        X.append([vocab.index(key[0]), 0, 0, 0, 0])\n",
    "        Y.append(vocab.index(value[i]))\n",
    "print('Done')\n",
    "for key, value in next_word2.items():\n",
    "    for i in range(len(value)):\n",
    "        writer.writerow([[vocab.index(key[0]), vocab.index(key[1]) , 0, 0, 0], \n",
    "                         vocab.index(value[i])])\n",
    "        X.append([vocab.index(key[0]), vocab.index(key[1]), 0, 0, 0])\n",
    "        Y.append(vocab.index(value[i]))\n",
    "print('Done')\n",
    "for key, value in next_word3.items():\n",
    "    for i in range(len(value)):\n",
    "        writer.writerow([[vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), 0 , 0], \n",
    "                         vocab.index(value[i])])\n",
    "        X.append([vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), 0, 0])\n",
    "        Y.append(vocab.index(value[i]))\n",
    "print('Done')\n",
    "for key, value in next_word4.items():\n",
    "    for i in range(len(value)):\n",
    "        writer.writerow([[vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), vocab.index(key[3]), 0], \n",
    "                         vocab.index(value[i])])\n",
    "        X.append([vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), vocab.index(key[3]), 0])\n",
    "        Y.append(vocab.index(value[i]))\n",
    "print('Done')\n",
    "for key, value in next_word5.items():\n",
    "    for i in range(len(value)):\n",
    "        writer.writerow([[vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), vocab.index(key[3]), vocab.index(key[4])], \n",
    "                         vocab.index(value[i])])\n",
    "        X.append([vocab.index(key[0]), vocab.index(key[1]), vocab.index(key[2]), vocab.index(key[3]), vocab.index(key[4])])\n",
    "        Y.append(vocab.index(value[i]))\n",
    "print('Done')\n",
    "\n",
    "data_set.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "administrative-operations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/379 [..............................] - ETA: 0s - loss: 7.4999WARNING:tensorflow:From /home/serenella.valiani/venv/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "379/379 [==============================] - ETA: 0s - loss: 6.1495\n",
      "Epoch 00001: loss improved from inf to 6.14950, saving model to nextword1.h5\n",
      "379/379 [==============================] - 43s 114ms/step - loss: 6.1495\n",
      "Epoch 2/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 5.7837\n",
      "Epoch 00002: loss improved from 6.14950 to 5.78373, saving model to nextword1.h5\n",
      "379/379 [==============================] - 42s 110ms/step - loss: 5.7837\n",
      "Epoch 3/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 5.4754\n",
      "Epoch 00003: loss improved from 5.78373 to 5.47541, saving model to nextword1.h5\n",
      "379/379 [==============================] - 41s 108ms/step - loss: 5.4754\n",
      "Epoch 4/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 5.1918\n",
      "Epoch 00004: loss improved from 5.47541 to 5.19182, saving model to nextword1.h5\n",
      "379/379 [==============================] - 41s 107ms/step - loss: 5.1918\n",
      "Epoch 5/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 4.8495\n",
      "Epoch 00005: loss improved from 5.19182 to 4.84950, saving model to nextword1.h5\n",
      "379/379 [==============================] - 41s 108ms/step - loss: 4.8495\n",
      "Epoch 6/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 4.4297\n",
      "Epoch 00006: loss improved from 4.84950 to 4.42967, saving model to nextword1.h5\n",
      "379/379 [==============================] - 41s 107ms/step - loss: 4.4297\n",
      "Epoch 7/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 3.9636\n",
      "Epoch 00007: loss improved from 4.42967 to 3.96362, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 107ms/step - loss: 3.9636\n",
      "Epoch 8/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 3.4804\n",
      "Epoch 00008: loss improved from 3.96362 to 3.48041, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 3.4804\n",
      "Epoch 9/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 3.0060\n",
      "Epoch 00009: loss improved from 3.48041 to 3.00601, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 3.0060\n",
      "Epoch 10/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 2.5864\n",
      "Epoch 00010: loss improved from 3.00601 to 2.58641, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 2.5864\n",
      "Epoch 11/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 2.2222\n",
      "Epoch 00011: loss improved from 2.58641 to 2.22222, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 2.2222\n",
      "Epoch 12/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.9222\n",
      "Epoch 00012: loss improved from 2.22222 to 1.92216, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.9222\n",
      "Epoch 13/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.7086\n",
      "Epoch 00013: loss improved from 1.92216 to 1.70863, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.7086\n",
      "Epoch 14/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.5462\n",
      "Epoch 00014: loss improved from 1.70863 to 1.54619, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.5462\n",
      "Epoch 15/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.4167\n",
      "Epoch 00015: loss improved from 1.54619 to 1.41672, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.4167\n",
      "Epoch 16/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.3285\n",
      "Epoch 00016: loss improved from 1.41672 to 1.32847, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.3285\n",
      "Epoch 17/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.2699\n",
      "Epoch 00017: loss improved from 1.32847 to 1.26994, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.2699\n",
      "Epoch 18/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.2164\n",
      "Epoch 00018: loss improved from 1.26994 to 1.21635, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.2164\n",
      "Epoch 19/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.1839\n",
      "Epoch 00019: loss improved from 1.21635 to 1.18389, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.1839\n",
      "Epoch 20/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.1500\n",
      "Epoch 00020: loss improved from 1.18389 to 1.14995, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 1.1500\n",
      "Epoch 21/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.1217\n",
      "Epoch 00021: loss improved from 1.14995 to 1.12172, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.1217\n",
      "Epoch 22/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0973\n",
      "Epoch 00022: loss improved from 1.12172 to 1.09726, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0973\n",
      "Epoch 23/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0741\n",
      "Epoch 00023: loss improved from 1.09726 to 1.07414, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0741\n",
      "Epoch 24/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0618\n",
      "Epoch 00024: loss improved from 1.07414 to 1.06185, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0618\n",
      "Epoch 25/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0524\n",
      "Epoch 00025: loss improved from 1.06185 to 1.05237, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0524\n",
      "Epoch 26/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0331\n",
      "Epoch 00026: loss improved from 1.05237 to 1.03306, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0331\n",
      "Epoch 27/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0257\n",
      "Epoch 00027: loss improved from 1.03306 to 1.02573, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0257\n",
      "Epoch 28/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0223\n",
      "Epoch 00028: loss improved from 1.02573 to 1.02230, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0223\n",
      "Epoch 29/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0184\n",
      "Epoch 00029: loss improved from 1.02230 to 1.01836, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0184\n",
      "Epoch 30/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 1.0003\n",
      "Epoch 00030: loss improved from 1.01836 to 1.00025, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 1.0003\n",
      "Epoch 31/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9918\n",
      "Epoch 00031: loss improved from 1.00025 to 0.99181, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9918\n",
      "Epoch 32/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9907\n",
      "Epoch 00032: loss improved from 0.99181 to 0.99074, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9907\n",
      "Epoch 33/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9912\n",
      "Epoch 00033: loss did not improve from 0.99074\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9912\n",
      "Epoch 34/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9776\n",
      "Epoch 00034: loss improved from 0.99074 to 0.97763, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9609\n",
      "Epoch 00035: loss improved from 0.97763 to 0.96091, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9609\n",
      "Epoch 36/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9575\n",
      "Epoch 00036: loss improved from 0.96091 to 0.95752, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9575\n",
      "Epoch 37/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9583\n",
      "Epoch 00037: loss did not improve from 0.95752\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9583\n",
      "Epoch 38/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9544\n",
      "Epoch 00038: loss improved from 0.95752 to 0.95437, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9544\n",
      "Epoch 39/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9474\n",
      "Epoch 00039: loss improved from 0.95437 to 0.94742, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9474\n",
      "Epoch 40/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9523\n",
      "Epoch 00040: loss did not improve from 0.94742\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9523\n",
      "Epoch 41/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9445\n",
      "Epoch 00041: loss improved from 0.94742 to 0.94450, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 107ms/step - loss: 0.9445\n",
      "Epoch 42/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9375\n",
      "Epoch 00042: loss improved from 0.94450 to 0.93746, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9375\n",
      "Epoch 43/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9394\n",
      "Epoch 00043: loss did not improve from 0.93746\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9394\n",
      "Epoch 44/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9313\n",
      "Epoch 00044: loss improved from 0.93746 to 0.93126, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 107ms/step - loss: 0.9313\n",
      "Epoch 45/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9267\n",
      "Epoch 00045: loss improved from 0.93126 to 0.92668, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 107ms/step - loss: 0.9267\n",
      "Epoch 46/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9130\n",
      "Epoch 00046: loss improved from 0.92668 to 0.91301, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9130\n",
      "Epoch 47/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9162\n",
      "Epoch 00047: loss did not improve from 0.91301\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.9162\n",
      "Epoch 48/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9149\n",
      "Epoch 00048: loss did not improve from 0.91301\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9149\n",
      "Epoch 49/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.9168\n",
      "Epoch 00049: loss did not improve from 0.91301\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "379/379 [==============================] - 40s 105ms/step - loss: 0.9168\n",
      "Epoch 50/50\n",
      "379/379 [==============================] - ETA: 0s - loss: 0.8379\n",
      "Epoch 00050: loss improved from 0.91301 to 0.83789, saving model to nextword1.h5\n",
      "379/379 [==============================] - 40s 106ms/step - loss: 0.8379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f81e40ccb70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PASSO 6\n",
    "#Definisco il modello\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "#data = pd.read_csv(\"dataset.csv\", names = [\"input\", \"output\"])\n",
    "#X = data['input']\n",
    "#Y = data['output']\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "Y = to_categorical(Y, num_classes=vocab_size)\n",
    "\n",
    "\n",
    "model = tensorflow.keras.Sequential()\n",
    "model.add(tensorflow.keras.layers.Embedding(vocab_size, 10, input_length=5))\n",
    "model.add(tensorflow.keras.layers.LSTM(1000, return_sequences=True))\n",
    "model.add(tensorflow.keras.layers.LSTM(1000))\n",
    "model.add(tensorflow.keras.layers.Dense(1000, activation=\"relu\"))\n",
    "model.add(tensorflow.keras.layers.Dense(vocab_size, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(lr=0.001))\n",
    "model.fit(X, Y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line, using only lowercase letters: she was a \n",
      "['she', 'was', 'a']\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#carico il modello e il tokenizer\n",
    "model = load_model('nextword1.h5')\n",
    "tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "    #text = text.split()\n",
    "    #print(text)\n",
    "    #print(type(text))\n",
    "    #for i in range(5):\n",
    "    #    sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "    #    sequence = np.array(sequence)\n",
    "    #    \n",
    "    #    preds = model.predict_classes(sequence)\n",
    "    #    predicted_word = \"\"\n",
    "\n",
    "\n",
    "    list_text = text.split()\n",
    "    print(list_text)\n",
    "    list_sequence = [0, 0, 0, 0, 0 ]\n",
    "    print(list_sequence)\n",
    "    sequence = tokenizer.text_to_sequence([text])[0]\n",
    "    print(sequence)\n",
    "    split_sequence = sequence.split()\n",
    "    print(split_sequence)\n",
    "    for i in range(len(split_sequence)):\n",
    "        print(split_sequence[i])\n",
    "        list_sequence[i] = split_sequence[i]\n",
    "    print(list_sequence)\n",
    "    #for i in range(len(list_text)):\n",
    "    #    print(list_sequence[i])\n",
    "    #    print(list_text[i])\n",
    "    #    list_sequence[i] = tokenizer.text_to_sequence([list_text[i]])[0]\n",
    "    #    print(list_sequence[i])\n",
    "    print(list_sequence)\n",
    "    sequence = np.array(list_sequence)\n",
    "    print(sequence)\n",
    "    preds = model.predict_classes(sequence)\n",
    "    predicted_word = ''\n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == preds:\n",
    "            predicted_word = key\n",
    "            break\n",
    "\n",
    "        print(predicted_word)\n",
    "        return predicted_word\n",
    "\n",
    "while(True):\n",
    "\n",
    "    text = input(\"Enter your line, using only lowercase letters: \")\n",
    "\n",
    "    if text == \"exit\":\n",
    "        print(\"Ending The Program.....\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            #text = text.split(\" \")\n",
    "            #text = text[-1]\n",
    "\n",
    "            #text = ''.join(text)\n",
    "            Predict_Next_Words(model, tokenizer, text)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-stylus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
